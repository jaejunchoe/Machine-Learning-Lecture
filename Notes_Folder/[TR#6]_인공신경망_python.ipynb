{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "(train_input, train_target), (test_input, test_target) = keras.datasets.mnist.load_data(path=\"mnist.npz\")\n"
      ],
      "metadata": {
        "id": "_2ZYYLM9dw-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0. MNIST DIGITS 데이터의 shape를 확인합니다.\n",
        "print(train_input.shape, train_target.shape)\n",
        "print(test_input.shape, test_target.shape)"
      ],
      "metadata": {
        "id": "JhMypwfHeBOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. train_input 데이터를 Neural Network에 사용하기 위해 변환해줍니다.\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "train_scaled = train_input / 255.0\n",
        "train_scaled = train_scaled.reshape(-1, 28*28)\n",
        "\n",
        "# Neural Networks 모델에서 펼치고 싶을 때\n",
        "# model.add(keras.layers.Flatten(input_shape=(28, 28)))"
      ],
      "metadata": {
        "id": "DDvcbJ4neR9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 하이퍼파라미터 셋팅을 위해 train 데이터를 train 과 validation 으로 분류합니다. 이 때 validation data의 비율은 20%, random state는 30으로 지정합니다.\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_scaled, val_scaled, train_target, val_target = train_test_split(train_scaled, train_target, test_size=0.2, random_state=30)\n",
        "\n",
        "print(train_scaled.shape, train_target.shape)\n",
        "print(val_scaled.shape, val_target.shape)\n"
      ],
      "metadata": {
        "id": "fb86Cm9ie3iA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. DNN 모델을 정의합니다. 모델은 모두 Sequential으로 정의합니다.\n",
        "\n",
        "# 3.1. 은닉층 없이 입력층에서 출력으로 이어지는 fully connected 층을 정의\n",
        "model1=keras.Sequential()\n",
        "model1.add(keras.layers.Dense(10, activation='softmax', input_shape=(784,)))\n",
        "\n",
        "# 3.2. 은닉층 1개 (뉴런 100개), 출력으로 이어지는 fully connected 층을 정의\n",
        "model3_2=keras.Sequential()\n",
        "model3_2.add(keras.layers.Dense(100, activation='relu', input_shape=(784,)))\n",
        "model3_2.add(keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# 3.3. 은닉층 2개 (뉴런 100개 / 뉴런 50개), 출력으로 이어지는 fully connected 층을 정의\n",
        "model3_3=keras.Sequential()\n",
        "model3_3.add(keras.layers.Dense(100, activation='relu', input_shape=(784,)))\n",
        "model3_3.add(keras.layers.Dense(50, activation='relu',))\n",
        "model3_3.add(keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# 3.4. 각 팀에서 원하는 대로 은닉층을 늘려서 모델을 정의\n",
        "# 은닉층 3개(뉴런 100개 / 뉴런 50개 / 뉴런 20개), 출력으로 이어지는 fully connected 층을 정의\n",
        "model3_4=keras.Sequential()\n",
        "model3_4.add(keras.layers.Dense(100, activation='relu', input_shape=(784,)))\n",
        "model3_4.add(keras.layers.Dense(50, activation='relu'))\n",
        "model3_4.add(keras.layers.Dense(20, activation='relu'))\n",
        "model3_4.add(keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# 3.5. 각 모델별 파라미터 개수를 확인하세요. model.summary() 기능 활용\n",
        "model1.summary()\n",
        "model3_2.summary()\n",
        "model3_3.summary()\n",
        "model3_4.summary()\n",
        "\n"
      ],
      "metadata": {
        "id": "7rBaruVofehl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_scaled.shape"
      ],
      "metadata": {
        "id": "GIoqTZVYpJtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 각 모델의 Compile 정의를 수행합니다. 이 때 아래와 같은 하이퍼파라미터를 설정합니다.\n",
        "model1.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model3_2.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "model3_3.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "model3_4.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "# P1 = model1.fit(train_scaled, train_target, epochs=10, batch_size=32)\n",
        "# P3_2 = model3_2.fit(train_scaled, train_target, epochs=10, batch_size=32)\n",
        "# P3_3 = model3_3.fit(train_scaled, train_target, epochs=10, batch_size=32)\n",
        "# P3_4 = model3_4.fit(train_scaled, train_target, epochs=10, batch_size=32)\n",
        "\n",
        "# 5. 각 모델을 학습해봅니다. 학습 시 train_scaled, train_target을 사용합니다. 또한 validation_data도 지정합니다.\n",
        "# 각 모델별 epoch 10회 수행 후 loss/accuracy와 val_loss/val_accuracy를 확인합니다.\n",
        "result1 = model1.fit(train_scaled, train_target, epochs=10, validation_data=(val_scaled, val_target))\n",
        "result3_2 = model3_2.fit(train_scaled, train_target, epochs=10, validation_data=(val_scaled, val_target))\n",
        "result3_3 = model3_3.fit(train_scaled, train_target, epochs=10, validation_data=(val_scaled, val_target))\n",
        "result3_4 = model3_4.fit(train_scaled, train_target, epochs=10, validation_data=(val_scaled, val_target))\n",
        "\n"
      ],
      "metadata": {
        "id": "cOx7mUlIhvcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # 각 모델별 값이 어떻게 다른지 확인하고, 그 이유를 찾아보세요.\n",
        "\n",
        " # loss에 해당하는 값은 점점 작아지고, accuracy에 해당하는값은 점점 증가하게 됩니다.\n",
        " # 그 이유는 epoch를 점차 진행할수록 모델이 점점 고도화되면서 학습량이 늘어나기 때문입니다."
      ],
      "metadata": {
        "id": "u-kEAZFuzaIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# epoch를 늘리면 값이 달라지나요? batch_size를 바꾸면 달라지나요?\n",
        "    # epoch를 20으로 변경해보세요.\n",
        "    # batch_size를 64로 변경해보세요.\n",
        "\n",
        "result2 = model1.fit(train_scaled, train_target, epochs=20, batch_size=64, validation_data=(val_scaled, val_target))\n",
        "result4_2 = model3_2.fit(train_scaled, train_target, epochs=20, batch_size=64, validation_data=(val_scaled, val_target))\n",
        "result4_3 = model3_3.fit(train_scaled, train_target, epochs=20, batch_size=64, validation_data=(val_scaled, val_target))\n",
        "result4_4 = model3_4.fit(train_scaled, train_target, epochs=20, batch_size=64, validation_data=(val_scaled, val_target))"
      ],
      "metadata": {
        "id": "8r3j---lzxdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# epoch를 늘리고 batch_size를 바꾼 후 값은 달라집니다. 그 이유는 기존보다 epcoh를 늘렸기에 당연히 loss에 해당하는 값과 accuracy에 해당하는 값이 기존보다 더 좋은 값을 갖습니다."
      ],
      "metadata": {
        "id": "9EVnXTQBzV0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 모델 5개에 대한 accuracy, loss 결과값 (epoch 10일 때)을 제출하세요.\n",
        "\n",
        "model3_5=keras.Sequential()\n",
        "model3_5.add(keras.layers.Dense(100, activation='relu', input_shape=(784,)))\n",
        "model3_5.add(keras.layers.Dense(50, activation='relu'))\n",
        "model3_5.add(keras.layers.Dense(20, activation='relu'))\n",
        "model3_5.add(keras.layers.Dense(15, activation='relu'))\n",
        "model3_5.add(keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model3_5.summary()\n",
        "\n",
        "model3_5.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "result3_5 = model3_5.fit(train_scaled, train_target, epochs=10, validation_data=(val_scaled, val_target))\n",
        "\n",
        "result4_5 = model3_5.fit(train_scaled, train_target, epochs=20, batch_size=64, validation_data=(val_scaled, val_target))\n"
      ],
      "metadata": {
        "id": "2pITEISI018d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}